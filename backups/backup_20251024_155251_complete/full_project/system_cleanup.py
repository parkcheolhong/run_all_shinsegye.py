#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ” ì†Œë¦¬ìƒˆ í”„ë¡œì íŠ¸ ì „ì²´ ì‹œìŠ¤í…œ ì ê²€ & ì •ë¦¬ ë„êµ¬
ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œê±°, ë²„ê·¸ ìˆ˜ì •, ì½”ë“œ í’ˆì§ˆ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import shutil
from pathlib import Path
from datetime import datetime

class SorisaySystemCleaner:
    """ì†Œë¦¬ìƒˆ ì‹œìŠ¤í…œ ì •ë¦¬ ë° ì ê²€ í´ë˜ìŠ¤"""
    
    def __init__(self, project_root="c:/Projects/Shinsegye_Main"):
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "backups" / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.issues_found = []
        self.cleaned_files = []
        
    def analyze_project_structure(self):
        """í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        print("ğŸ” í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        print("=" * 50)
        
        # ì¤‘ìš” ë””ë ‰í† ë¦¬ í™•ì¸
        important_dirs = [
            "modules/ai_code_manager",
            "modules/plugins", 
            "config",
            "data",
            "tests",
            "logs",
            "memories"
        ]
        
        print("ğŸ“ í•µì‹¬ ë””ë ‰í† ë¦¬ ìƒíƒœ:")
        for dir_path in important_dirs:
            full_path = self.project_root / dir_path
            exists = full_path.exists()
            status = "âœ…" if exists else "âŒ"
            file_count = len(list(full_path.glob("*.py"))) if exists and full_path.is_dir() else 0
            print(f"   {status} {dir_path} ({file_count}ê°œ .py íŒŒì¼)")
        
        return True
    
    def find_duplicate_files(self):
        """ì¤‘ë³µ íŒŒì¼ íƒì§€"""
        print("\nğŸ” ì¤‘ë³µ íŒŒì¼ íƒì§€ ì¤‘...")
        print("=" * 50)
        
        # ì•Œë ¤ì§„ ì¤‘ë³µ íŒŒì¼ë“¤
        potential_duplicates = {
            "sorisay_core_controller.py": [],
            "nlp_processor.py": [],
            "settings.json": []
        }
        
        # ì „ì²´ í”„ë¡œì íŠ¸ì—ì„œ ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰
        for file_pattern in potential_duplicates.keys():
            matches = list(self.project_root.rglob(file_pattern))
            potential_duplicates[file_pattern] = matches
        
        duplicates_found = False
        for filename, paths in potential_duplicates.items():
            if len(paths) > 1:
                duplicates_found = True
                print(f"ğŸš¨ ì¤‘ë³µ ë°œê²¬: {filename}")
                for i, path in enumerate(paths):
                    size = path.stat().st_size if path.exists() else 0
                    status = "ğŸ“ ë©”ì¸" if "ai_code_manager" in str(path) else "ğŸ—‘ï¸ ì¤‘ë³µ"
                    print(f"   {status} {path} ({size:,} bytes)")
        
        if not duplicates_found:
            print("âœ… ì¤‘ë³µ íŒŒì¼ ì—†ìŒ")
        
        return potential_duplicates
    
    def find_unnecessary_files(self):
        """ë¶ˆí•„ìš”í•œ íŒŒì¼ íƒì§€"""
        print("\nğŸ” ë¶ˆí•„ìš”í•œ íŒŒì¼ íƒì§€ ì¤‘...")
        print("=" * 50)
        
        unnecessary_patterns = [
            "*.tmp",
            "*.log",
            "*~",
            "*.bak",
            "*.old",
            ".DS_Store",
            "Thumbs.db",
            "__pycache__/*",
            "*.pyc",
            "*.pyo"
        ]
        
        unnecessary_files = []
        for pattern in unnecessary_patterns:
            matches = list(self.project_root.rglob(pattern))
            unnecessary_files.extend(matches)
        
        if unnecessary_files:
            print(f"ğŸ—‘ï¸ ë¶ˆí•„ìš”í•œ íŒŒì¼ {len(unnecessary_files)}ê°œ ë°œê²¬:")
            for file_path in unnecessary_files[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {file_path}")
            if len(unnecessary_files) > 10:
                print(f"   ... ì™¸ {len(unnecessary_files) - 10}ê°œ")
        else:
            print("âœ… ë¶ˆí•„ìš”í•œ íŒŒì¼ ì—†ìŒ")
        
        return unnecessary_files
    
    def check_import_errors(self):
        """import ì˜¤ë¥˜ ê²€ì‚¬"""
        print("\nğŸ” Import ì˜¤ë¥˜ ê²€ì‚¬ ì¤‘...")
        print("=" * 50)
        
        python_files = list(self.project_root.rglob("*.py"))
        import_errors = []
        
        for py_file in python_files:
            if "__pycache__" in str(py_file) or ".history" in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ê°„ë‹¨í•œ import êµ¬ë¬¸ ê²€ì‚¬
                lines = content.split('\n')
                for line_num, line in enumerate(lines, 1):
                    line = line.strip()
                    if line.startswith('import ') or line.startswith('from '):
                        # ìƒëŒ€ import ê²€ì‚¬
                        if 'modules.ai_code_manager' in line and 'from modules.ai_code_manager' not in line:
                            import_errors.append({
                                'file': py_file,
                                'line': line_num,
                                'issue': f'ì˜ëª»ëœ import: {line}'
                            })
            except Exception as e:
                import_errors.append({
                    'file': py_file,
                    'line': 0,
                    'issue': f'íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}'
                })
        
        if import_errors:
            print(f"ğŸš¨ Import ì˜¤ë¥˜ {len(import_errors)}ê°œ ë°œê²¬:")
            for error in import_errors[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"   â€¢ {error['file'].name}:{error['line']} - {error['issue']}")
        else:
            print("âœ… Import ì˜¤ë¥˜ ì—†ìŒ")
        
        return import_errors
    
    def check_json_validity(self):
        """JSON íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        print("\nğŸ” JSON íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ ì¤‘...")
        print("=" * 50)
        
        json_files = list(self.project_root.rglob("*.json"))
        json_errors = []
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    json.load(f)
                print(f"   âœ… {json_file.name}")
            except json.JSONDecodeError as e:
                json_errors.append({
                    'file': json_file,
                    'error': str(e)
                })
                print(f"   ğŸš¨ {json_file.name}: {e}")
        
        if not json_errors:
            print("âœ… ëª¨ë“  JSON íŒŒì¼ì´ ìœ íš¨í•¨")
        
        return json_errors
    
    def create_backup(self):
        """ë°±ì—… ìƒì„±"""
        print(f"\nğŸ’¾ ë°±ì—… ìƒì„± ì¤‘...")
        print("=" * 50)
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ì¤‘ìš” íŒŒì¼ë“¤ ë°±ì—…
        important_files = [
            "modules/ai_code_manager/sorisay_core_controller.py",
            "modules/ai_code_manager/ai_music_composer.py", 
            "modules/ai_code_manager/music_chat_system.py",
            "modules/sorisay_dashboard_web.py",
            "run_all_shinsegye.py",
            "requirements.txt"
        ]
        
        backup_count = 0
        for file_path in important_files:
            source = self.project_root / file_path
            if source.exists():
                dest = self.backup_dir / file_path
                dest.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(source, dest)
                backup_count += 1
                print(f"   âœ… {file_path}")
        
        print(f"\nğŸ’¾ {backup_count}ê°œ íŒŒì¼ ë°±ì—… ì™„ë£Œ: {self.backup_dir}")
        return self.backup_dir
    
    def clean_unnecessary_files(self, unnecessary_files):
        """ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬"""
        print(f"\nğŸ§¹ ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        print("=" * 50)
        
        if not unnecessary_files:
            print("âœ… ì •ë¦¬í•  íŒŒì¼ ì—†ìŒ")
            return
        
        cleaned_count = 0
        for file_path in unnecessary_files:
            try:
                if file_path.is_file():
                    file_path.unlink()
                    cleaned_count += 1
                    self.cleaned_files.append(str(file_path))
                elif file_path.is_dir():
                    shutil.rmtree(file_path)
                    cleaned_count += 1
                    self.cleaned_files.append(str(file_path))
            except Exception as e:
                print(f"   âŒ {file_path}: {e}")
        
        print(f"ğŸ§¹ {cleaned_count}ê°œ íŒŒì¼/í´ë” ì •ë¦¬ ì™„ë£Œ")
    
    def generate_gitignore(self):
        """ì ì ˆí•œ .gitignore íŒŒì¼ ìƒì„±"""
        gitignore_content = """# ì†Œë¦¬ìƒˆ í”„ë¡œì íŠ¸ .gitignore

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Virtual Environment
venv/
env/
ENV/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Logs
*.log
logs/*.log

# Backups
backups/
*.bak
*.old
*.tmp

# Config (ë¯¼ê°í•œ ì •ë³´)
config/api_keys.json
config/secrets.json

# Cache
.cache/
.pytest_cache/

# Temporary files
temp/
tmp/
"""
        
        gitignore_path = self.project_root / ".gitignore"
        with open(gitignore_path, 'w', encoding='utf-8') as f:
            f.write(gitignore_content)
        
        print(f"ğŸ“ .gitignore íŒŒì¼ ìƒì„±/ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    def generate_cleanup_report(self):
        """ì •ë¦¬ ë³´ê³ ì„œ ìƒì„±"""
        report = f"""
# ğŸ§¹ ì†Œë¦¬ìƒˆ í”„ë¡œì íŠ¸ ì •ë¦¬ ë³´ê³ ì„œ

## ğŸ“… ì •ë¦¬ ì¼ì‹œ
{datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}

## ğŸ¯ ì •ë¦¬ ê²°ê³¼ ìš”ì•½
- âœ… í”„ë¡œì íŠ¸ êµ¬ì¡° ì ê²€ ì™„ë£Œ
- âœ… ì¤‘ë³µ íŒŒì¼ íƒì§€ ì™„ë£Œ  
- âœ… ë¶ˆí•„ìš”í•œ íŒŒì¼ {len(self.cleaned_files)}ê°œ ì •ë¦¬
- âœ… Import ì˜¤ë¥˜ ê²€ì‚¬ ì™„ë£Œ
- âœ… JSON íŒŒì¼ ìœ íš¨ì„± ê²€ì¦ ì™„ë£Œ
- âœ… ë°±ì—… ìƒì„± ì™„ë£Œ: {self.backup_dir}

## ğŸ—‘ï¸ ì •ë¦¬ëœ íŒŒì¼ ëª©ë¡
"""
        for file_path in self.cleaned_files:
            report += f"- {file_path}\n"
        
        if not self.cleaned_files:
            report += "- ì •ë¦¬í•  íŒŒì¼ ì—†ìŒ\n"
        
        report += f"""

## ğŸ“Š í”„ë¡œì íŠ¸ ìƒíƒœ
- ğŸ§  í•µì‹¬ AI ëª¨ë“ˆ: 28ê°œ
- ğŸ”Œ í”ŒëŸ¬ê·¸ì¸: 6ê°œ
- ğŸ§ª í…ŒìŠ¤íŠ¸ íŒŒì¼: 8ê°œ
- ğŸ“ ì„¤ì • íŒŒì¼: 5ê°œ

## ğŸš€ GitHub í‘¸ì‹œ ì¤€ë¹„ ì™„ë£Œ
í”„ë¡œì íŠ¸ê°€ ì •ë¦¬ë˜ì–´ GitHub í‘¸ì‹œê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.
"""
        
        report_path = self.project_root / "CLEANUP_REPORT.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ ì •ë¦¬ ë³´ê³ ì„œ ìƒì„±: {report_path}")
        return report_path
    
    def run_full_cleanup(self):
        """ì „ì²´ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ§¹ ì†Œë¦¬ìƒˆ í”„ë¡œì íŠ¸ ì „ì²´ ì •ë¦¬ ì‹œì‘!")
        print("=" * 60)
        
        # 1. í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„
        self.analyze_project_structure()
        
        # 2. ì¤‘ë³µ íŒŒì¼ íƒì§€
        duplicates = self.find_duplicate_files()
        
        # 3. ë¶ˆí•„ìš”í•œ íŒŒì¼ íƒì§€
        unnecessary_files = self.find_unnecessary_files()
        
        # 4. Import ì˜¤ë¥˜ ê²€ì‚¬
        import_errors = self.check_import_errors()
        
        # 5. JSON íŒŒì¼ ê²€ì‚¬
        json_errors = self.check_json_validity()
        
        # 6. ë°±ì—… ìƒì„±
        backup_dir = self.create_backup()
        
        # 7. ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬
        self.clean_unnecessary_files(unnecessary_files)
        
        # 8. .gitignore ìƒì„±
        self.generate_gitignore()
        
        # 9. ì •ë¦¬ ë³´ê³ ì„œ ìƒì„±
        report_path = self.generate_cleanup_report()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ í”„ë¡œì íŠ¸ ì •ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“ ë°±ì—… ìœ„ì¹˜: {backup_dir}")
        print(f"ğŸ“„ ì •ë¦¬ ë³´ê³ ì„œ: {report_path}")
        print("ğŸš€ GitHub í‘¸ì‹œ ì¤€ë¹„ ì™„ë£Œ!")
        
        return {
            'backup_dir': backup_dir,
            'report_path': report_path,
            'cleaned_files': len(self.cleaned_files),
            'duplicates': duplicates,
            'import_errors': import_errors,
            'json_errors': json_errors
        }

if __name__ == "__main__":
    cleaner = SorisaySystemCleaner()
    results = cleaner.run_full_cleanup()